"""
Dynamic Query Execution Engine for AI-Generated Queries

This service safely executes AI-generated SQL queries with comprehensive
safety controls, monitoring, and result formatting.
"""

import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import json

from django.utils import timezone
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError, TimeoutError as SQLTimeoutError

from ddpui.core.ai.query_generator import (
    QueryPlan,
    QueryValidationResult,
    NaturalLanguageQueryService,
)
from ddpui.core.ai.data_intelligence import DataIntelligenceService
from ddpui.models.org import Org, OrgWarehouse
from ddpui.datainsights.warehouse.warehouse_factory import WarehouseFactory
from ddpui.utils.custom_logger import CustomLogger

logger = CustomLogger("ddpui.core.ai.query_executor")


@dataclass
class QueryExecutionResult:
    """Result of executing an AI-generated query"""

    success: bool
    data: List[Dict[str, Any]] = field(default_factory=list)
    columns: List[str] = field(default_factory=list)
    row_count: int = 0
    execution_time_ms: int = 0
    error_message: str = ""
    warnings: List[str] = field(default_factory=list)
    query_plan: Optional[QueryPlan] = None
    executed_at: datetime = field(default_factory=timezone.now)

    # Security and monitoring
    was_query_modified: bool = False
    original_query: str = ""
    executed_query: str = ""
    resource_usage: Dict[str, Any] = field(default_factory=dict)
    source_tables: List[str] = field(default_factory=list)


@dataclass
class ExecutionSafetyLimits:
    """Safety limits for query execution"""

    max_execution_time_seconds: int = 30
    max_result_rows: int = 1000
    max_memory_mb: int = 100
    max_cpu_seconds: int = 15
    allowed_query_types: List[str] = field(default_factory=lambda: ["SELECT"])

    # Rate limiting
    max_queries_per_minute: int = 10
    max_queries_per_hour: int = 60


class DynamicQueryExecutor:
    """
    Safe execution engine for AI-generated SQL queries.

    Provides comprehensive safety controls, monitoring, and result formatting
    for queries generated by the NaturalLanguageQueryService.
    """

    def __init__(self):
        self.logger = CustomLogger("DynamicQueryExecutor")
        self.query_generator = NaturalLanguageQueryService()
        self.data_intelligence = DataIntelligenceService()
        self.safety_limits = ExecutionSafetyLimits()

        # Query execution tracking
        self._execution_history = []
        self._rate_limit_tracker = {}

    def execute_natural_language_query(
        self,
        question: str,
        org: Org,
        dashboard_id: Optional[int] = None,
        user_context: Optional[Dict[str, Any]] = None,
    ) -> QueryExecutionResult:
        """
        Execute a natural language query with comprehensive safety controls.

        Args:
            question: User's natural language question
            org: Organization context
            dashboard_id: Optional dashboard for focused analysis
            user_context: Additional user context for personalization

        Returns:
            QueryExecutionResult with execution status and data
        """
        self.logger.info(
            f"Executing natural language query for org {org.slug}: {question[:100]}..."
        )

        execution_start = time.time()

        try:
            # Rate limiting check
            rate_limit_result = self._check_rate_limits(org.id)
            if not rate_limit_result[0]:
                return QueryExecutionResult(
                    success=False, error_message=rate_limit_result[1], query_plan=None
                )

            # Generate query plan from natural language
            query_plan = self.query_generator.generate_query_from_question(
                question=question, org=org, dashboard_id=dashboard_id
            )

            # Early exit if query generation failed
            if not query_plan.requires_execution or not query_plan.generated_sql:
                return QueryExecutionResult(
                    success=False,
                    error_message=query_plan.explanation,
                    query_plan=query_plan,
                    execution_time_ms=int((time.time() - execution_start) * 1000),
                )

            # Validate query meets safety requirements
            if query_plan.validation_result and not query_plan.validation_result.is_valid:
                return QueryExecutionResult(
                    success=False,
                    error_message=f"Query validation failed: {query_plan.validation_result.error_message}",
                    query_plan=query_plan,
                    execution_time_ms=int((time.time() - execution_start) * 1000),
                )

            # Execute the validated query
            execution_result = self._execute_validated_query(
                query_plan=query_plan, org=org, execution_start=execution_start
            )

            # Update rate limiting tracker
            self._update_rate_limit_tracker(org.id)

            # Log execution for monitoring
            self._log_query_execution(query_plan, execution_result, org)

            return execution_result

        except Exception as e:
            self.logger.error(f"Error executing natural language query: {e}")
            return QueryExecutionResult(
                success=False,
                error_message=f"Execution failed: {str(e)}",
                execution_time_ms=int((time.time() - execution_start) * 1000),
            )

    def _execute_validated_query(
        self, query_plan: QueryPlan, org: Org, execution_start: float
    ) -> QueryExecutionResult:
        """Execute a validated query with safety controls"""
        try:
            # Get warehouse connection
            org_warehouse = OrgWarehouse.objects.filter(org=org).first()
            if not org_warehouse:
                return QueryExecutionResult(
                    success=False,
                    error_message="No warehouse configured for organization",
                    query_plan=query_plan,
                    execution_time_ms=int((time.time() - execution_start) * 1000),
                )

            warehouse_client = WarehouseFactory.get_warehouse_client(org_warehouse)

            # Apply final safety modifications to query
            safe_query, modifications = self._apply_safety_modifications(query_plan.generated_sql)

            # Set execution timeout
            query_start = time.time()

            # Execute query with timeout and safety controls
            results = self._execute_with_safety_controls(
                warehouse_client=warehouse_client,
                query=safe_query,
                timeout_seconds=self.safety_limits.max_execution_time_seconds,
            )

            execution_time_ms = int((time.time() - query_start) * 1000)

            # Process and format results
            formatted_data, columns = self._format_query_results(results)

            # Apply result size limits
            if len(formatted_data) > self.safety_limits.max_result_rows:
                formatted_data = formatted_data[: self.safety_limits.max_result_rows]
                modifications.append(
                    f"Results limited to {self.safety_limits.max_result_rows} rows"
                )

            # Capture source tables from validation if available
            source_tables: List[str] = []
            if query_plan.validation_result and query_plan.validation_result.tables_accessed:
                source_tables = list(query_plan.validation_result.tables_accessed)

            return QueryExecutionResult(
                success=True,
                data=formatted_data,
                columns=columns,
                row_count=len(formatted_data),
                execution_time_ms=execution_time_ms,
                warnings=modifications,
                query_plan=query_plan,
                was_query_modified=len(modifications) > 0,
                original_query=query_plan.generated_sql,
                executed_query=safe_query,
                resource_usage={
                    "execution_time_ms": execution_time_ms,
                    "result_rows": len(formatted_data),
                    "result_columns": len(columns),
                },
                source_tables=source_tables,
            )

        except SQLTimeoutError:
            return QueryExecutionResult(
                success=False,
                error_message=f"Query timeout after {self.safety_limits.max_execution_time_seconds} seconds",
                query_plan=query_plan,
                execution_time_ms=int((time.time() - execution_start) * 1000),
            )
        except SQLAlchemyError as e:
            return QueryExecutionResult(
                success=False,
                error_message=f"Database error: {str(e)}",
                query_plan=query_plan,
                execution_time_ms=int((time.time() - execution_start) * 1000),
            )
        except Exception as e:
            self.logger.error(f"Unexpected error during query execution: {e}")
            return QueryExecutionResult(
                success=False,
                error_message=f"Execution error: {str(e)}",
                query_plan=query_plan,
                execution_time_ms=int((time.time() - execution_start) * 1000),
            )

    def _apply_safety_modifications(self, query: str) -> Tuple[str, List[str]]:
        """Apply final safety modifications to the query"""
        modifications = []
        safe_query = query.strip()

        # Ensure LIMIT clause exists and is within bounds
        if "LIMIT" not in safe_query.upper():
            safe_query += f" LIMIT {self.safety_limits.max_result_rows}"
            modifications.append(f"Added LIMIT {self.safety_limits.max_result_rows} for safety")
        else:
            # Check if existing LIMIT exceeds our safety limit
            import re

            limit_match = re.search(r"LIMIT\s+(\d+)", safe_query, re.IGNORECASE)
            if limit_match:
                limit_value = int(limit_match.group(1))
                if limit_value > self.safety_limits.max_result_rows:
                    safe_query = re.sub(
                        r"LIMIT\s+\d+",
                        f"LIMIT {self.safety_limits.max_result_rows}",
                        safe_query,
                        flags=re.IGNORECASE,
                    )
                    modifications.append(
                        f"Reduced LIMIT to {self.safety_limits.max_result_rows} for safety"
                    )

        # Ensure query ends with semicolon for clean execution
        if not safe_query.rstrip().endswith(";"):
            safe_query += ";"

        return safe_query, modifications

    def _execute_with_safety_controls(
        self, warehouse_client, query: str, timeout_seconds: int
    ) -> List[Any]:
        """Execute query with comprehensive safety controls"""
        try:
            # Create parameterized query
            sql_query = text(query)

            # Execute with timeout
            # Note: Actual timeout implementation depends on the warehouse client
            # This is a simplified version - production should use connection-level timeouts
            results = warehouse_client.execute(sql_query)

            return results

        except Exception as e:
            self.logger.error(f"Error executing query with safety controls: {e}")
            raise

    def _format_query_results(
        self, raw_results: List[Any]
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """Format raw query results for consistent consumption"""
        if not raw_results:
            return [], []

        # Handle different result formats from different warehouse types
        formatted_data = []
        columns = []

        try:
            first_row = raw_results[0]

            if isinstance(first_row, dict):
                # Results are already dictionaries
                columns = list(first_row.keys())
                formatted_data = [
                    {col: self._safe_format_value(row.get(col)) for col in columns}
                    for row in raw_results
                ]
            else:
                # Results are tuples/lists - need column names
                # This is a simplified approach - in production, get column names from result metadata
                columns = [f"column_{i}" for i in range(len(first_row))]
                formatted_data = [
                    {f"column_{i}": self._safe_format_value(value) for i, value in enumerate(row)}
                    for row in raw_results
                ]

            return formatted_data, columns

        except Exception as e:
            self.logger.error(f"Error formatting query results: {e}")
            # Return safe fallback
            return [{"error": f"Result formatting failed: {str(e)}"}], ["error"]

    def _safe_format_value(self, value: Any) -> Any:
        """Safely format a value for JSON serialization"""
        if value is None:
            return None
        elif isinstance(value, (str, int, float, bool)):
            return value
        elif hasattr(value, "isoformat"):  # datetime objects
            return value.isoformat()
        else:
            # Convert complex types to string representation
            return str(value)[:1000]  # Limit string length for safety

    def _check_rate_limits(self, org_id: int) -> Tuple[bool, str]:
        """Check if organization has exceeded rate limits"""
        current_time = timezone.now()

        if org_id not in self._rate_limit_tracker:
            self._rate_limit_tracker[org_id] = {"queries_this_minute": [], "queries_this_hour": []}

        org_tracker = self._rate_limit_tracker[org_id]

        # Clean old entries
        minute_ago = current_time.timestamp() - 60
        hour_ago = current_time.timestamp() - 3600

        org_tracker["queries_this_minute"] = [
            t for t in org_tracker["queries_this_minute"] if t > minute_ago
        ]
        org_tracker["queries_this_hour"] = [
            t for t in org_tracker["queries_this_hour"] if t > hour_ago
        ]

        # Check limits
        if len(org_tracker["queries_this_minute"]) >= self.safety_limits.max_queries_per_minute:
            return (
                False,
                f"Rate limit exceeded: max {self.safety_limits.max_queries_per_minute} queries per minute",
            )

        if len(org_tracker["queries_this_hour"]) >= self.safety_limits.max_queries_per_hour:
            return (
                False,
                f"Rate limit exceeded: max {self.safety_limits.max_queries_per_hour} queries per hour",
            )

        return True, ""

    def _update_rate_limit_tracker(self, org_id: int):
        """Update rate limiting tracker after successful execution"""
        current_time = timezone.now().timestamp()

        if org_id not in self._rate_limit_tracker:
            self._rate_limit_tracker[org_id] = {"queries_this_minute": [], "queries_this_hour": []}

        org_tracker = self._rate_limit_tracker[org_id]
        org_tracker["queries_this_minute"].append(current_time)
        org_tracker["queries_this_hour"].append(current_time)

    def _log_query_execution(
        self, query_plan: QueryPlan, execution_result: QueryExecutionResult, org: Org
    ):
        """Log query execution for monitoring and analytics"""
        log_data = {
            "org_id": org.id,
            "org_slug": org.slug,
            "question": query_plan.original_question,
            "generated_sql": query_plan.generated_sql,
            "execution_success": execution_result.success,
            "execution_time_ms": execution_result.execution_time_ms,
            "result_row_count": execution_result.row_count,
            "ai_confidence": query_plan.confidence_score,
            "query_complexity": query_plan.validation_result.complexity_score
            if query_plan.validation_result
            else None,
            "was_modified": execution_result.was_query_modified,
            "error_message": execution_result.error_message
            if not execution_result.success
            else None,
            "warnings": execution_result.warnings,
            "executed_at": execution_result.executed_at.isoformat(),
        }

        # Log at appropriate level based on success
        if execution_result.success:
            self.logger.info(f"Query executed successfully: {log_data}")
        else:
            self.logger.warning(f"Query execution failed: {log_data}")

        # Store in execution history for debugging
        self._execution_history.append(log_data)

        # Keep only recent history to prevent memory growth
        if len(self._execution_history) > 100:
            self._execution_history = self._execution_history[-100:]

    def get_execution_stats(self, org_id: Optional[int] = None) -> Dict[str, Any]:
        """Get execution statistics for monitoring"""
        if org_id:
            relevant_executions = [e for e in self._execution_history if e["org_id"] == org_id]
        else:
            relevant_executions = self._execution_history

        if not relevant_executions:
            return {"total_executions": 0}

        successful_executions = [e for e in relevant_executions if e["execution_success"]]

        return {
            "total_executions": len(relevant_executions),
            "successful_executions": len(successful_executions),
            "success_rate": len(successful_executions) / len(relevant_executions),
            "average_execution_time_ms": sum(e["execution_time_ms"] for e in successful_executions)
            / len(successful_executions)
            if successful_executions
            else 0,
            "average_result_rows": sum(e["result_row_count"] for e in successful_executions)
            / len(successful_executions)
            if successful_executions
            else 0,
            "average_ai_confidence": sum(e["ai_confidence"] for e in relevant_executions)
            / len(relevant_executions),
            "common_errors": self._get_common_errors(relevant_executions),
            "rate_limits_active": len(self._rate_limit_tracker) > 0,
        }

    def _get_common_errors(self, executions: List[Dict]) -> List[Dict[str, Any]]:
        """Analyze common error patterns"""
        error_counts = {}

        for execution in executions:
            if not execution["execution_success"] and execution["error_message"]:
                error_type = (
                    execution["error_message"].split(":")[0]
                    if ":" in execution["error_message"]
                    else execution["error_message"]
                )
                error_counts[error_type] = error_counts.get(error_type, 0) + 1

        # Return top 5 most common errors
        return [
            {"error_type": error, "count": count}
            for error, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        ]
